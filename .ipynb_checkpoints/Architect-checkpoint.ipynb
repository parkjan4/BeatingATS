{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beating the Applicant Tracking System (ATS): Architect\n",
    "This project idea first occurred to me when I was about to refine my resume. Most companies now use some sort of applicant tracking system (ATS) to filter out applicants whose resumes do not include enough of the \"keywords\" that matter to the job profile. Generally, people attempt to look through a few examples of job descriptions out there in order to arrive at an array of importants words or phrases that they must include in order to stand out. What they cannot do is look through potentially thousands of job descriptions and come up with a much more accurate, comprehensive list (and ranking!) of keywords tailored for the kind of job they are applying for. This is precisely the overarching goal of this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Web scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# NLP\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "os.environ.update({'MALLET_HOME':r'C:/new_mallet/mallet-2.0.8/'})\n",
    "mallet_path = 'C:/new_mallet/mallet-2.0.8/bin/mallet'\n",
    "\n",
    "# Other scikit-learn packages\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD, PCA, KernelPCA\n",
    "\n",
    "# Visualization\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I. Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we sort the jobs in order of relevance. Indeed has the tendency to sort chronologically after the sorting, which is also what we want. Also by default, we search within 25 km of the requested city of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define some helper functions. A big shout-out to this [medium post](https://medium.com/@msalmon00/web-scraping-job-postings-from-indeed-96bd588dcb4b) for already defining a number of useful functions for parsing html data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PAGES = 100 # max. number of pages to search (1 page = ~20 jobs)\n",
    "\n",
    "def extract_job_titles(soup): \n",
    "    jobs = []\n",
    "    for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "        for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "            jobs.append(a['title'])\n",
    "    return jobs\n",
    "\n",
    "def extract_job_desc(soup):\n",
    "    job_desc = []\n",
    "    progress = 0\n",
    "    for div in soup.find_all(name='div', attrs={'class':'title'}):\n",
    "        for a in div.find_all(name='a'):\n",
    "            # Enter the job description page\n",
    "            job_desc_page = requests.get('https://indeed.ca/viewjob?jk=' + a['id'][3:])\n",
    "            \n",
    "            # Get description content\n",
    "            desc = BeautifulSoup(job_desc_page.text, 'html.parser')\n",
    "            desc_text = desc.find_all(name='div', attrs={'id':'jobDescriptionText'})[0].text\n",
    "            job_desc.append(desc_text)\n",
    "    return job_desc\n",
    "\n",
    "def extract_companies(soup): \n",
    "    companies = []\n",
    "    for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "        company = div.find_all(name='span', attrs={'class':'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name='span', attrs={'class':'result-link-source'})\n",
    "            for span in sec_try:\n",
    "                companies.append(span.text.strip())\n",
    "    return companies\n",
    "\n",
    "def search_pages(job, city, country, starting_page=\"&start=0\"):\n",
    "    job_search_title = \"+\".join([w for w in job.split()])\n",
    "    city_search_title = \"+\".join([w for w in city.split()])\n",
    "    \n",
    "    # Define URL to search on Indeed\n",
    "    if country=='US':\n",
    "        URL_head = \"https://www.indeed.com/jobs?q=\"\n",
    "    elif country=='CA':\n",
    "        URL_head = \"https://www.indeed.ca/jobs?q=\"\n",
    "    else:\n",
    "        print('Invalid country')\n",
    "        raise\n",
    "\n",
    "    URL = URL_head + job_search_title + \"&l=\" + city_search_title + \"&sort=relevance\" + starting_page\n",
    "\n",
    "    # Get search page\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser') # page data in HTML format\n",
    "    \n",
    "    job_titles = extract_job_titles(soup)\n",
    "    return job_titles, extract_companies(soup), extract_job_desc(soup) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define job title of interest\n",
    "jobs = ['architect']\n",
    "\n",
    "# Define city of interest\n",
    "cities = ['Toronto','Vancouver','Ottawa','Hamilton','Calgary','Edmonton','Winnipeg'] # Canadian major cities\n",
    "country = 'CA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROGERSS = 1.29%\r"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "job_titles = []\n",
    "job_coms = []\n",
    "job_desc = []\n",
    "\n",
    "progress = 0\n",
    "prev_jt = None\n",
    "\n",
    "for p in itertools.product(jobs, cities):\n",
    "    job = p[0]\n",
    "    city = p[1]\n",
    "    for n in range(NUM_PAGES):\n",
    "        print('PROGERSS = {:.2f}%'.format(100*(progress+1)/NUM_PAGES/(len(jobs)*len(cities))), end=\"\\r\")\n",
    "        progress += 1\n",
    "\n",
    "        if n==0:\n",
    "            jt, jc, jd = search_pages(job, city, country, starting_page=\"&start=\"+str(n))\n",
    "        else:\n",
    "            jt, jc, jd = search_pages(job, city, country, starting_page=\"&start=\"+str(int(n*len(jt)))) # increment by number of jobs in a page\n",
    "\n",
    "        if len(jt)==0:\n",
    "            print(job.upper()+' '+city.upper()+': '+'no more results... terminating search.')\n",
    "            break \n",
    "        elif jt==prev_jt:\n",
    "            print(job.upper()+' '+city.upper()+': '+'no more new results... terminating search.')\n",
    "            break \n",
    "        else:\n",
    "            prev_jt = jt \n",
    "            job_titles += jt\n",
    "            job_coms += jc\n",
    "            job_desc += jd\n",
    "    \n",
    "print('Number of jobs found: ', len(job_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Title': job_titles, 'Company': job_coms, 'Description': job_desc})\n",
    "print('Size: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Data Cleaning & Processing (NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "df['Description'] = df['Description'].apply(lambda x: tokenizer.tokenize(x)) \n",
    "\n",
    "# Lower case all letters\n",
    "df['Description'] = df['Description'].apply(lambda x: [w.lower() for w in x])\n",
    "df.head()\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "df['Description'] = df['Description'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"a\") for w in x])\n",
    "df['Description'] = df['Description'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"v\") for w in x])\n",
    "df['Description'] = df['Description'].apply(lambda x: [lemmatizer.lemmatize(w, pos=\"n\") for w in x])\n",
    "\n",
    "# Stopwords\n",
    "stop_words = list(set(stopwords.words('english'))) # standard list of stop words\n",
    "\n",
    "# Common action verbs to use as stop words\n",
    "text = open('common_action_verbs.txt', 'r')\n",
    "custom_words = []\n",
    "for w in text.readlines():\n",
    "    try:\n",
    "        word = w.split()[0][:-1].lower()\n",
    "        word = lemmatizer.lemmatize(word, pos='v')\n",
    "        custom_words.append(word)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "more_custom_words = ['qualifications','require','requirements','consider','work','position','candidate','apply','successful','good','join',\\\n",
    "                     'job','experience','need','ubisoft','provide','support','meet','ensure','cv','resume','cover','letter','cbi']\n",
    "\n",
    "# Customize stop words\n",
    "stop_words.extend(custom_words)\n",
    "stop_words.extend(more_custom_words)\n",
    "\n",
    "job_words = []\n",
    "for j in jobs:\n",
    "    tmp = j.split()\n",
    "    job_words.append(tmp[0].lower())\n",
    "    job_words.append(tmp[1].lower())\n",
    "stop_words.extend(job_words)\n",
    "city_words = []\n",
    "for c in cities:\n",
    "    city_words.append(c.lower())\n",
    "stop_words.extend(city_words)\n",
    "\n",
    "# Remove stopwords\n",
    "df['Description'] = df['Description'].apply(lambda x: ' '.join([w for w in x if w not in stop_words]))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. N-grams and Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for x in df['Description']:\n",
    "    corpus.append(x)\n",
    "    \n",
    "def get_ngrams(n,max_num=30):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n,n), max_features=2000).fit(corpus)\n",
    "    bow = vectorizer.transform(corpus)\n",
    "    sum_words = bow.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True) # descending\n",
    "    return words_freq[:max_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud\n",
    "one_giant_text_string = ''\n",
    "for c in corpus:\n",
    "    one_giant_text_string = one_giant_text_string + ' ' + c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(one_giant_text_string)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1-, 2-, 3-gram models\n",
    "keywords = get_ngrams(1)\n",
    "key_phrases1 = get_ngrams(2)\n",
    "key_phrases2 = get_ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_and_counts(vector):\n",
    "    words = []\n",
    "    counts = []\n",
    "    for x in vector:\n",
    "        words.append(x[0])\n",
    "        counts.append(x[1])\n",
    "    return words, counts\n",
    "\n",
    "def barh_plotter(words, counts):\n",
    "    sns.set(style='whitegrid', font_scale=1.5)\n",
    "    sns.color_palette(\"RdBu\", n_colors=7)\n",
    "    fig, ax = plt.subplots(figsize=(13,8))\n",
    "    ax = sns.barplot(x='Count',y='Word',data=pd.DataFrame({'Word':words,'Count':counts}))\n",
    "    \n",
    "    ax.set_ylabel('Keywords')\n",
    "    ax.set_xlabel('Frequency')\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords\n",
    "words, counts = get_words_and_counts(keywords)\n",
    "ax = barh_plotter(words, counts)\n",
    "ax.set_title('Keyword Ranking',fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# Key phrases\n",
    "words, counts = get_words_and_counts(key_phrases1)\n",
    "ax = barh_plotter(words, counts)\n",
    "ax.set_title('2-Word-Phrase Ranking',fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "words, counts = get_words_and_counts(key_phrases2)\n",
    "ax = barh_plotter(words, counts)\n",
    "ax.set_title('3-Word-Phrase Ranking',fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Dimensionality Reduction and Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorizer with bigrams\n",
    "text = list(df['Description'].values)\n",
    "tfidf = CountVectorizer(ngram_range=(1,2))\n",
    "# tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "X = tfidf.fit_transform(text)\n",
    "X.shape, type(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(0.2*X.shape[0])\n",
    "svd = TruncatedSVD(n_components=N, random_state=42).fit(X.transpose())\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.bar(range(1,N+1,1), np.cumsum(100*svd.explained_variance_ratio_))\n",
    "plt.ylabel('Percentage of variance explained')\n",
    "plt.xlabel('Components')\n",
    "plt.title('Cumulative percentage explained in SVD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the number of components used above explain enough variability of the original feature space despite being smaller by more than half. This also has the additional benefit of making T-SNE much faster (a nonlinear dimensionality reduction technique which we clearly need based on the numbers directly above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dimensionality reduction using TSNE\n",
    "X_svd = svd.transform(X.transpose())\n",
    "\n",
    "kl = {}\n",
    "embedding = {} # #words x #components\n",
    "progress = 0\n",
    "\n",
    "p=30\n",
    "tsne = TSNE(n_components=2, perplexity=p, random_state=42, n_jobs=-1).fit(X_svd) # default hyperparameters\n",
    "embedding[p] = tsne.embedding_\n",
    "kl[p] = tsne.kl_divergence_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,9))\n",
    "X_embedded = embedding[p]\n",
    "ax.scatter(X_embedded[:,0], X_embedded[:,1], s=1)\n",
    "ax.set_ylabel('x$_2$')\n",
    "ax.set_xlabel('x$_1$')\n",
    "ax.set_title('TSNE Embedding (Perplexity='+str(p)+')')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fit DBSCAN (find appropriate hyperparameters based on the plots above)\n",
    "opt_p = p\n",
    "X_embedded = embedding[opt_p]\n",
    "dbscan = DBSCAN(eps=1.5, n_jobs=-1).fit(X_embedded) \n",
    "ids = dbscan.labels_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot color-coded clusters on TNSE embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,9))\n",
    "ax.scatter(X_embedded[:,0], X_embedded[:,1], c=ids.astype(float), s=1)\n",
    "ax.set_ylabel('x$_2$')\n",
    "ax.set_xlabel('x$_1$')\n",
    "ax.set_title('Clustered TSNE Embedding (Perplexity='+str(opt_p)+')')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V. Visualize with Real Word Annotations\n",
    "\n",
    "The original TSNE embedding just has too many data points to annotate with realy words as text. Here, a few clusters are presented separately with actual word annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get clusters by \"zone\"\n",
    "zone = [(-50,50), (-50,50)] # x,y limits\n",
    "\n",
    "idx = (X_embedded[:,0]>=zone[0][0])\\\n",
    "      &(X_embedded[:,0]<=zone[0][1])\\\n",
    "      &(X_embedded[:,1]>=zone[1][0])\\\n",
    "      &(X_embedded[:,1]<=zone[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(40,40))\n",
    "\n",
    "#idx = ids==-1\n",
    "sub_X = X_embedded[idx,:]\n",
    "ax.scatter(sub_X[:,0], sub_X[:,1], alpha=0.5, c=ids[idx].astype(float))\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_title('TSNE Embedding (Cluster ID: '+str(c)+')')\n",
    "list_coords = []\n",
    "for (w,j) in tfidf.vocabulary_.items():\n",
    "    if idx[j]==False:\n",
    "        continue\n",
    "    curr_coord = np.array((X_embedded[j,0], X_embedded[j,1]))\n",
    "    if len(list_coords)==0:\n",
    "        ax.annotate(w, curr_coord, rotation=0, fontsize=18) # Annotate with real words\n",
    "        list_coords.append(curr_coord)\n",
    "    else:\n",
    "        # Find if previously occupied coordinates is \"too close\"\n",
    "        euc_dist = np.array([np.sqrt((curr_coord-z).T@(curr_coord-z)) for z in list_coords])\n",
    "        condition = euc_dist <= 1.5\n",
    "        if sum(condition) > 0:\n",
    "            continue\n",
    "        else:\n",
    "            ax.annotate(w, curr_coord, rotation=0, fontsize=18) # Only annotate \"non-overlapping points\"\n",
    "            list_coords.append(curr_coord)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A. Attempt at LDA Topic Modeling (did not get meaningful topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build bigram and trigram models\n",
    "bigram = gensim.models.Phrases(df['Description']) \n",
    "trigram = gensim.models.Phrases(bigram[df['Description']])  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run = 0\n",
    "if run:\n",
    "    df_lda = make_bigrams(df['Description']) \n",
    "\n",
    "    # Define corpus\n",
    "    id2word = corpora.Dictionary(df_lda)\n",
    "    corpus_lda = [id2word.doc2bow(text) for text in df_lda]\n",
    "\n",
    "    # Optimize LDA model\n",
    "    scores = {}\n",
    "    models = {}\n",
    "    progress = 0\n",
    "    for K in np.arange(2,15,1):\n",
    "        print('PROGERSS = {:.2f}%'.format(100*(progress+1)/len(np.arange(3,16,1))), end=\"\\r\")\n",
    "        progress += 1\n",
    "\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus_lda, id2word=id2word, num_topics=K, random_state=42)\n",
    "        models[K] = lda_model\n",
    "\n",
    "        # Compute coherence score\n",
    "        cm = CoherenceModel(model=lda_model, texts=df_lda, dictionary=id2word, coherence='c_v')\n",
    "        scores[K] = cm.get_coherence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run:\n",
    "    fig, ax = plt.subplots(figsize=(9,6))\n",
    "    ax.plot(list(scores.keys()), list(scores.values()))\n",
    "    ax.set_ylabel('Coherence score', fontsize=15)\n",
    "    ax.set_xlabel('Number of topics', fontsize=15)\n",
    "    ax.tick_params(axis='both', labelsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if run:\n",
    "    opt_K = 6\n",
    "    opt_model = models[opt_K]\n",
    "\n",
    "    # Visualize topics-keywords\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(opt_model, corpus_lda, id2word)\n",
    "    vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
